<!-- Coggle 30 Days of ML（22年3月） -->
<!-- 30天入门数据竞赛 -->
<!-- 2022-01-01 -->
<!-- <a target="_blank" href="https://www.zhihu.com/people/ashui233/">阿水</a>, <a target="_blank" href="https://www.zhihu.com/people/wang-he-13-93">鱼遇雨欲语与余</a>-->
<!-- <a href="https://coggle.club/blog/30days-of-ml-202201">学习资料</a>##<a href="https://shimo.im/forms/vZyk3Pvmc7kvAskG/fill">打卡链接</a> -->


## Part1 内容介绍

在给大家分享知识的过程中，发现很多同学在工作和学业中存在较多的问题：

* 对网络编程了解较少，不会从HTML中提取信息；
* 不会爬虫，不会收集数据，也不会部署模型；
* 不了解Spark，不会使用Spark进行数据处理。


而上述问题都是一个合格算法工程师所必备的。因此我们将从本月组织一次竞赛训练营活动，希望能够帮助大家入门数据科学。在活动中我们将布置具体任务，然后参与的同学们不断闯关完成，竟可能的帮助大家入门。


---


## Part2 活动安排

* 活动是免费学习活动，不会收取任何费用。
* **请各位同学添加下面微信，并回复【竞赛学习】，即可参与。**

![](https://cdn.coggle.club/coggle666_qrcode.png)

---

## Part3 积分说明和奖励

为了激励各位同学完成的学习任务，将学习任务根据难度进行划分，并根据是否完成进行评分难度高中低的任务分别分数为3、2和1。在完成3月学习后（本次活动，截止3月底），将按照积分顺序进行评选 Top3 的学习者。

打卡链接：[https://shimo.im/forms/SDujCSsLOjMgbZLQ/fill](https://shimo.im/forms/SDujCSsLOjMgbZLQ/fill)

| 微信昵称 | 爬虫得分 | Spark得分 | 总得分 |
| -------- | ------------ | --------------- | ------ |
| lixh2100 |  |  |  |
| 无盐 | | 3 | 3|
| soufal | | 4 | 4 | 
| 人墙 | | 1 | 1 |
| 冯玉博 | | 2| 2 |
| yyds | 1 | | 1 |
| 无题 | | | |
| Good Lucky | | | |
| 小胡 | | | |
| tobebetter | | 链接打不开 | |

**打卡可以写在一个地址，每次有新完成的可以重复提交打卡！**

Top1的学习者将获得以下**奖励**：
* Coggle 竞赛专访机会
* 《机器学习算法竞赛实战》，鱼佬签名版


Top2-3的学习者将获得以下**奖励**：
* Coggle 周边福利
* Coggle 竞赛专访机会
*使用PaddlePaddle完成学习的Top3同学，还可以领取百度提供的小礼物。*


注：
* Coggle 数据科学保留活动期间和结束后修改奖励和规则的权利。
* 如果有违反竞赛规则的情况，Coggle 数据科学保留取消相关参赛者的参与排名的权利。

---

## Part4 爬虫与网络编程基础

### 学习内容

当今的世界是一个互联的世界，绝大多数的计算机和人都在通过网络和他人传递信息、沟通互联。我们在网络上学习、游戏、工作，我们提供各种各样的网络服务，又有很多人使用着各种各样的网络服务。网络改变了世界，而程序员“定义”了网络。我们在代码中实现了网络的通信，让一切变得可能。网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。**在本次学习中我们将学习基础的爬虫操作，并学习基础的HTTP协议，最后尝试完成基础的网络编程。**

### 打卡汇总

| 任务名称                       | 难度  | 所需技能  |
| :----------------------------- | :---- | :-------- |
| 任务1：计算机网络基础          | 低、1 | json、xml |
| 任务2：HTTP协议与requests      | 低、1 | requests  |
| 任务3：bs4基础使用             | 中、2 | bs4       |
| 任务4：bs4高阶使用             | 高、3 | bs4       |
| 任务5：正则表达式              | 高、3 | re        |
| 任务6：Python网络编程基础      | 高、3 | scoket    |
| 任务7：tornado基础使用         | 中、2 | tornado   |
| 任务8：tornado用户注册/登录    | 高、3 | tornado   |
| 任务9：tornado部署机器学习模型 | 中、2 | tornado   |

- 任务1：计算机网络基础
    - 步骤1：在Pyhon中创建一个list，存储以下个人信息（姓名、年龄、成绩）：[小王、40、50]，[小贾、50、23]
    - 步骤2：将步骤1的数据存储为json格式，并进行读取
    - 步骤3：将步骤1的数据存储为xml格式，并进行读取
    - 步骤4：学习[计算机网络基础](https://www.runoob.com/w3cnote/summary-of-network.html)，思考从打开[coggle.club](https://coggle.club/)到网页展示，有什么步骤？将你的思考结果写到博客。

- 任务2：HTTP协议与requests
    - 步骤1：[学习HTTP协议](https://www.cnblogs.com/an-wen/p/11180076.html)
    - 步骤2：HTTP的get和post有什么区别？用处在哪儿？将你的思考写到博客。
    - 步骤3：使用Python中requests中的get访问百度。

- 任务3：bs4基础使用
    - 学习资料：[https://beautiful-soup-4.readthedocs.io/en/latest/](https://beautiful-soup-4.readthedocs.io/en/latest/)
    - 步骤1：使用requests和bs4爬取[sklearn api页面](https://scikit-learn.org/stable/modules/classes.html)
    - 步骤2：在api页面中有多少个模块？有多少个API？如sklearn.base.DensityMixin，其中base为模块，DensityMixin为API
    - 步骤3：将模块名作为key，api作为value存储为字典。

- 任务4：bs4高阶使用 
    - 步骤1：爬取[sklearn 机器学习名词页面](https://scikit-learn.org/stable/glossary.html), 爬取所有的名词，如1d、2d和api；
    - 步骤2：有一些名词在介绍时，会有额外的链接，请将每个名词介绍对应的链接也找出。
    - 步骤3：原始的名词安装本身有类别，如下所示，你能将爬取的结果进行分类吗？

```
General Concepts
Class APIs and Estimator Types
Target Types
Methods
Parameters
Attributes
Data and sample properties
```

- 任务5：正则表达式
    - 步骤1：[学习正则表达式re模块使用](https://www.runoob.com/python/python-reg-expressions.html)。
    - 步骤2：使用re筛选出机器学习名词，只包含字母的名词；
    - 步骤3：使用re筛选出机器学习名词，首字母为A 或 首字母为B的名词；

- 任务6：Python网络编程基础
    - 步骤1：[学习Socket编程](https://www.runoob.com/python/python-socket.html)
    - 步骤2：使用编写一个Socket聊天机器人，程序A发送数据给程序B，程序B也可以发送信息给程序A；
    - 步骤3：使用编写一个Socket聊天机器人，程序A发送文件内容给程序B，程序B将文件进行存储；


- 任务7：tornado基础使用
    - 步骤1：学习tornado基础使用
        - tornado官网：[https://www.tornadoweb.org/en/stable/](https://www.tornadoweb.org/en/stable/)
        - tornado教程：
            - [http://www.ttlsa.com/docs/tornado/](http://www.ttlsa.com/docs/tornado/)
            - [http://doc.iplaypy.com/tornado/ch1.html](http://doc.iplaypy.com/tornado/ch1.html)
    - 步骤2：编写tornado 的 hello word程序
    - 步骤3：编写tornado 的handler，分别接受post和get请求，请求为两个数字，进行求和，然后返回结果。

- 任务8：tornado用户注册/登录
    - 步骤1：使用sqlite创建用户信息表，表包含`uid`，`name`，`passwd`三个字段。
    - 步骤2：编写tornado 的用户注册handler，完成用户注册逻辑，具体需要判断用户名和passwd合理性（不包含空格 & 最大长度限制），然后插入数据。
    - 步骤3：编写tornado 的用户登录handler，完成用户登录逻辑，根据name和passwd判断是否登录成功。
    - 步骤4：结合requests和tornado完成上述逻辑。

- 任务9：tornado部署机器学习模型
    - 步骤1：读取外卖评论数据集，[https://mirror.coggle.club/dataset/waimai_10k.csv](https://mirror.coggle.club/dataset/waimai_10k.csv)
    - 步骤2：使用jieba进行分词，TFIDF提取特征，并选择分类器进行训练。
    - 步骤3：将文本分类模型使用tornado进行部署，客户端requests发送文本进行分类。

### 学习资料

- [https://www.jianshu.com/p/f6292d732217](https://www.jianshu.com/p/f6292d732217)
- [https://www.bilibili.com/video/av59706997/](https://www.bilibili.com/video/av59706997/)
- [https://cuiqingcai.com/1319.html](https://cuiqingcai.com/1319.html)

### 打卡要求

**注：**

* **需要所有的任务可以写在一个博客内**
* **推荐在打卡过程中加入思考过程，可以加入尝试&资料记录**

---

## Part5 Spark基础教程

### 学习内容

Spark是一个快速和通用的大数据引擎，可以通俗的理解成一个分布式的大数据处理框架，允许用户将Spark部署在大量廉价的硬件之上，形成集群。Spark使用scala 实现，提供了 JAVA, Python，R等语言的调用接口。**在本次学习我们将学习如何使用Spark清洗数据，并进行基础的特征工程操作，帮助大家掌握基础PySpark技能。**

### 打卡汇总

| 任务名称                           | 难度  |
| :--------------------------------- | :---- |
| 任务1：PySpark数据处理   | 低、1 |
| 任务2：PySpark数据统计 | 中、1 |
| 任务3：PySpark分组聚合         | 中、2 |
| 任务4：SparkSQL基础语法            | 高、3 |
| 任务5：SparkML基础：数据编码       | 中、3 |
| 任务6：SparkML基础：分类模型       | 中、3 |
| 任务7：SparkML基础：聚类模型           | 中、2 |
| 任务8：Spark RDD                   | 高、3 |
| 任务9：Spark Streaming             | 高、2 |

> 环境说明：同学可以使用本地spark环境，参考spark进行安装。如果想使用我们学习环境，请联系小助手。

- 任务1：PySpark数据处理
    - 步骤1：使用Python链接Spark环境
    - 步骤2：创建dateframe数据
    - 步骤3：用spark执行以下逻辑：找到数据行数、列数
    - 步骤4：用spark筛选class为1的样本
    - 步骤5：用spark筛选language >90 或 math> 90的样本

```
import pandas as pd
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName('pyspark') \
    .getOrCreate()
# 原始数据 
test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),
                            ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),
                            ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],                           ['number','class','language','math','english','physic','chemical'])
test.show()
```
   
- 任务2：PySpark数据统计
    - 步骤1：读取文件[https://cdn.coggle.club/Pokemon.csv](https://cdn.coggle.club/Pokemon.csv)
    - 步骤2：将读取的进行保存，表头也需要保存
    - 步骤3：分析每列的类型，取值个数
    - 步骤4：分析每列是否包含缺失值

```
from pyspark import SparkFiles
spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')
df = spark.read.csv("file://"+SparkFiles.get("Pokemon.csv"), header=True, inferSchema= True)
df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')
df = df.withColumnRenamed('Sp. Def', 'Sp Def')
```


- 任务3：PySpark分组聚合
    - 步骤1：读取文件[https://cdn.coggle.club/Pokemon.csv](https://cdn.coggle.club/Pokemon.csv)
    - 步骤2：学习groupby分组聚合的使用
    - 步骤3：学习agg分组聚合的使用
    - 步骤4：学习transform的使用
    - 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值

- 任务4：SparkSQL基础语法
    - 步骤1：使用Spark SQL完成任务1里面的数据筛选
    - 步骤2：使用Spark SQL完成任务2里面的统计
    - 步骤3：使用Spark SQL完成任务3的分组统计

- 任务5：SparkML基础：数据编码
    - 步骤1：学习Spark ML中数据编码模块
    - 步骤2：读取文件[Pokemon.csv](https://cdn.coggle.club/Pokemon.csv)，理解数据[字段含义](https://www.kaggle.com/rounakbanik/pokemon)
    - 步骤3：将其中的类别属性使用`onehotencoder`
    - 步骤4：对其中的数值属性字段使用`minmaxscaler`
    - 步骤5：对编码后的属性使用`pca`进行降维（维度可以自己选择）

- 任务6：SparkML基础：分类模型
    - 步骤1：继续任务5的步骤，假设`Type 1`为标签，将其进行`labelencoder`
    - 步骤2：导入合适的标签评价指标，说出选择的原因？
    - 步骤3：选择至少3种分类方法，完成训练。

- 任务7：SparkML基础：聚类模型
    - 步骤1：继续任务5的步骤，假设`Type 1`为标签，将其进行`labelencoder`
    - 步骤2：使用kmeans对宝可梦进行聚类，使用肘部法选择合适聚类个数。

- 任务8：Spark RDD
    - 步骤1：学习资料[https://spark.apache.org/docs/latest/rdd-programming-guide.html#basics]
    - 步骤2：使用RDD functions完成任务2的统计逻辑。

- 任务9：Spark Streaming
    - 步骤1：学习资料[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
    - 步骤2：读取文件[https://cdn.coggle.club/Pokemon.csv](https://cdn.coggle.club/Pokemon.csv)为textFileStream
    - 步骤3：使用`filter`筛选行不包含`Grass`的文本
    - 步骤4：使用`flatmap`对文本行进行拆分

```
spark = SparkSession.builder.appName("CrossCorrelation").getOrCreate()
ssc = StreamingContext(spark.sparkContext, 1)
ds = ssc.textFileStream(input_path)
```


### 学习资料

- [https://spark.apache.org/docs/latest/quick-start.html](https://spark.apache.org/docs/latest/quick-start.html)
- [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [https://github.com/apache/spark/tree/4f25b3f712/examples/src/main/python](https://github.com/apache/spark/tree/4f25b3f712/examples/src/main/python)
- [https://sparkbyexamples.com/pyspark-tutorial/](https://sparkbyexamples.com/pyspark-tutorial/)
- [https://www.tutorialspoint.com/pyspark/index.htm](https://www.tutorialspoint.com/pyspark/index.htm)

## Part6 提问&回答

问：具体的活动是怎么安排的？

> 有任务，自己先尝试。活动结束后会公开优秀打卡链接。

问：本次活动是收费的吗，最终奖品如何发放？

> 活动是免费的，最终奖品按照积分排行Top3进行发放，如果排名有并列都发送奖励。

问：环境和配置是什么？

> 推荐在AI Studio上进行学习，有python3和PaddlePaddle环境，提供免费GPU

问：AI Studio有什么学习资料？

> 项目环境介绍：[https://ai.baidu.com/ai-doc/AISTUDIO/Dk3e2vxg9](https://ai.baidu.com/ai-doc/AISTUDIO/Dk3e2vxg9)

> Notebook环境：[https://ai.baidu.com/ai-doc/AISTUDIO/sk3e2z8sb](https://ai.baidu.com/ai-doc/AISTUDIO/sk3e2z8sb)