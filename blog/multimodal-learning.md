<!-- å¤šæ¨¡æ€æœºå™¨å­¦ä¹  -->
<!--  -->
<!-- 2023-06-01 -->
<!-- <a target="_blank" href="https://www.zhihu.com/people/ashui233/">é˜¿æ°´</a>, <a target="_blank" href="https://www.zhihu.com/people/wang-he-13-93">é±¼é‡é›¨æ¬²è¯­ä¸ä½™</a>-->
<!--  -->

## Image Caption

### æ•°æ®é›†

- [AI Challengerå›¾åƒä¸­æ–‡æè¿°æ•°æ®é›†](https://tianchi.aliyun.com/dataset/145781)

æ•°æ®æ¥è‡ª2017 AI Challengerï¼Œæ•°æ®é›†å¯¹ç»™å®šçš„æ¯ä¸€å¼ å›¾ç‰‡æœ‰äº”å¥è¯çš„ä¸­æ–‡æè¿°ã€‚æ•°æ®é›†åŒ…å«30ä¸‡å¼ å›¾ç‰‡ï¼Œ150ä¸‡å¥ä¸­æ–‡æè¿°ã€‚è®­ç»ƒé›†ï¼š210,000 å¼ ï¼ŒéªŒè¯é›†ï¼š30,000 å¼ ï¼Œæµ‹è¯•é›† Aï¼š30,000 å¼ ï¼Œæµ‹è¯•é›† Bï¼š30,000 å¼ ã€‚

![](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/167688733288720471676887332492.png)

### è¯„ä»·æŒ‡æ ‡

### é¢†åŸŸæ¨¡å‹

[https://huggingface.co/models?pipeline_tag=image-to-text](https://huggingface.co/models?pipeline_tag=image-to-text)

#### Show, Attend, and Tell

[https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)

#### Image Captioning with PyTorch and Transformers ğŸ’»ğŸ’¥

[https://github.com/senadkurtisi/pytorch-image-captioning](https://github.com/senadkurtisi/pytorch-image-captioning)


#### å…¶ä»–è®ºæ–‡

- [Fine-grained Image Captioning with CLIP Reward](https://arxiv.org/pdf/2205.13115.pdf)

ä¼ ç»Ÿæ¨¡å‹é€šå¸¸ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼æ€§ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯ç”±äºå…¬å…±æ•°æ®é›†ä¸­çš„å‚è€ƒå­—å¹•é€šå¸¸åªæè¿°æœ€æ˜¾è‘—çš„å¸¸è§å¯¹è±¡ï¼Œä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼æ€§ç›®æ ‡çš„æ¨¡å‹å¾€å¾€ä¼šå¿½ç•¥å›¾åƒä¸­å…·ä½“å’Œè¯¦ç»†çš„æ–¹é¢ï¼Œè¿™äº›æ–¹é¢ä½¿å›¾åƒä»å…¶ä»–å›¾åƒä¸­åŒºåˆ†å¼€æ¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨CLIPä½œä¸ºå¥–åŠ±å‡½æ•°çš„å¤šæ¨¡æ€ç›¸ä¼¼æ€§è®¡ç®—æ–¹æ³•ï¼Œä»¥å®ç°æ›´å…·æè¿°æ€§å’ŒåŒºåˆ†åº¦çš„å­—å¹•ç”Ÿæˆï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„CLIPæ–‡æœ¬ç¼–ç å™¨å¾®è°ƒç­–ç•¥æ¥æ”¹å–„è¯­æ³•ï¼Œæ— éœ€é¢å¤–çš„æ–‡æœ¬æ³¨é‡Šã€‚

- [Unsupervised Image Captioning](https://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.pdf)

åœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹è®­ç»ƒå›¾åƒå­—å¹•Image Captioningã€‚ä¼ ç»Ÿæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡æˆå¯¹çš„å›¾åƒå’Œå¥å­æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§æ•°æ®é›†çš„è·å–éå¸¸æ˜‚è´µã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåªéœ€è¦ä¸€ä¸ªå›¾åƒé›†ã€ä¸€ä¸ªå¥å­è¯­æ–™åº“å’Œä¸€ä¸ªç°æœ‰çš„è§†è§‰æ¦‚å¿µæ£€æµ‹å™¨ï¼Œå°±å¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªå›¾åƒå­—å¹•ç”Ÿæˆæ¨¡å‹ã€‚

## Mathematical Expression Recognition

### é¢†åŸŸæ¨¡å‹

- ICDAR 2021, Handwritten Mathematical Expression Recognition with Bidirectionally Trained Transformer
- CVPR 2022, Syntax-Aware Network for Handwritten Mathematical Expression Recognition

[https://github.com/whywhs/Pytorch-Handwritten-Mathematical-Expression-Recognition](https://github.com/whywhs/Pytorch-Handwritten-Mathematical-Expression-Recognition)


### æ•°æ®é›†

- [IM2LATEX-100K](https://www.kaggle.com/datasets/shahrukhkhan/im2latex100k)

A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~100k formulas and images splitted into train, validation and test sets. 

- [K-12 æ‰‹å†™ä½“ï¼ˆHME100Kï¼‰](https://ai.100tal.com/dataset)

The HME100K dataset provides 99,109 images (74,502 for training and 24,607 for testing) with 245 kinds of symbol classes. The data size is increased tenfold compared to the CRHOME datasets. HME100K acquires the expressions from an Internet application. Therefore, we may assume that the expressions are written by tens of thousands of writers. 

- [CROHME](https://github.com/JianshuZhang/WAP/tree/master/data)