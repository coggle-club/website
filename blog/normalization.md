归一化 ( Normalization) 是一种简化计算的方式。 将所有属性以相同的测量单位表示，并使用通用的刻度或范围。归一化试图赋予所有数据属性同等的权重，使属性之间的比较与聚合更容易，数据的收敛速度更快。

## 数据预处理中的「归一化」

对数据执行预处理步骤将原始属性转换成能利用的属性，有助于模型的训练。

### Min-Max 归一化

Min-Max 归一化也称为线性函数归一化，对原始数据做一次线性变换，将原 据映射到 $[0,1]$ 之间，不改变原始数据分布。

$$
x_{\text {new }}=\frac{x-\text { Min }}{M a x-M i n}
$$

- 优点：并不改变数据分布。
- 缺点: 会受到异常数据音效

### Z-Score

Z-Score将原数据处理成符合正态分布的数据，与均值和标准差进行计算。

$$
x_{\text {new }}=\frac{x-\text { Mean }}{\text { StandardDeviation }}
$$

- 优点：受离群值影响较小，适合最大值、最小值末知的情况。
- 缺点：会改变数据的分布。


### Sigmoid
Sigmoid 函数也称为 Logistic 函数，输人的数据被映射在 $[0,1]$ 之间。

$$
F(x)=\frac{1}{1+e^{-x}}
$$

- 优点：不受异常值影响。
- 缺点：改变了原始数据分布形态。

### RankGauss

RankGauss先对要处理的数据进行排序，将目标数据转换尺度到 $[-1,1]$。然后将按照的sigmoid的逆函数还原为数值，使得归一化后的数据满足高斯分布。

- 优点：数据变为高斯分布, 更为直观。
- 缺点：只保留了数据的排序信息。

## 数据归一化层的「归一化」

深度学习下归一化方法需要考虑迭代训练的过程，需要不断按照批量数据进行归一化，并积累历史数据的规律。

### Batch Normalization

 Batch Normalization是在隐藏层的每一层输前加一个归一化层，先进行归一化处理，然后参与网络计算。

- 优点：提升收敛速度，在计算机视觉任务上表现较好。
- 缺点：依赖批量大小

### Layer Normalization

Layer Normalization是对当前隐藏层整层做归一化操作。与Batch Normalization的不同之处在于，BN是针对同一个样本中的所有数据，而 LN是针对于单个样本来操作。

- 优点: 批量大小较小时, 效果好，适用于自然语 言处理任务。
- 缺点: 批量大小较大时, 效果不如BN。