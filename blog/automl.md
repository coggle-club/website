<!-- æ—¶é—´åºåˆ— -->
<!-- æœ¬æ–‡æ•´ç†äº†æ—¶é—´åºåˆ—çš„çŸ¥è¯†ç‚¹ -->
<!-- 2022-01-08 -->
<!-- <a target="_blank" href="https://www.zhihu.com/people/ashui233/">é˜¿æ°´</a>, <a target="_blank" href="https://www.zhihu.com/people/wang-he-13-93">é±¼é‡é›¨æ¬²è¯­ä¸ä½™</a>-->
<!--  -->


æ·»åŠ ğŸ‘‡å¾®ä¿¡ï¼Œæ‹‰ä½ è¿›å…¥å­¦ä¹ ç¾¤ã€‚

![](https://cdn.coggle.club/coggle666_qrcode.png)

## å¸¸è§Pythonåº“

- Auto-Sklearn
    - ä¸»é¡µï¼š[https://www.ml4aad.org/automl/auto-sklearn/](https://www.ml4aad.org/automl/auto-sklearn/)
    - Githubï¼š[https://github.com/automl/auto-sklearn](https://github.com/automl/auto-sklearn)

åœ¨æœºå™¨å­¦ä¹ ä¸­è¶…å‚æ•°æ§åˆ¶æ¨¡å‹å¦‚ä½•è¡¨ç°ï¼Œä¹Ÿå½±å“äº†æ¨¡å‹çš„ç²¾åº¦ï¼Œè¶…å‚æ•°éœ€è¦æå‰è®¾ç½®ã€‚åœ¨sklearnä¸­ä¹Ÿå†…ç½®äº†Grid Searchå’ŒRandom Searchçš„è¶…å‚æ•°æœç´¢æ–¹æ³•ã€‚

**æœ¬æ–‡æ€»ç»“äº†Pythonç¯å¢ƒä¸‹å¸¸è§çš„è¶…å‚æ•°ä¼˜åŒ–åº“ï¼Œæ¬¢è¿æ”¶è—å’Œé˜…è¯»ã€‚**

## Ray-Tune
https://github.com/ray-project/tune-sklearn

## SigOpt

https://sigopt.com/

## SmartML

https://bigdata.cs.ut.ee/smartml/index.html

## Optuna
https://github.com/optuna/optuna

## Hyperopt
https://github.com/hyperopt/hyperopt-sklearn

https://github.com/hyperopt/hyperopt

## Metric Optimisation Engine (MOE)

https://github.com/Yelp/MOE

## mlmachine
https://github.com/petersontylerd/mlmachine#Installation

## Polyaxon
https://polyaxon.com/docs/automation/optimization-engine/

## Bayesian Optimization
https://github.com/fmfn/BayesianOptimization


## SHERPA
https://parameter-sherpa.readthedocs.io/en/latest/


## Scikit-Optimize
https://scikit-optimize.github.io/stable/user_guide.html

## GPyOpt

https://sheffieldml.github.io/GPyOpt/


åœ¨ä»Šå¹´KDDä¼šè®®ç°åœºï¼Œç”±é˜¿é‡ŒåŒå­¦åˆ†äº«äº†**AutoML: A Perspective where Industry Meets Academy**ï¼Œåˆ†äº«ä¸­å¯¹AutoMLåšäº†å·¥ä¸šæ¡ˆä¾‹çš„ä»‹ç»ï¼Œéå¸¸é€‚åˆå…¥é—¨å­¦ä¹ ã€‚

AutoMLä¸»è¦çš„è½åœ°æ–¹å‘å¦‚ä¸‹ï¼š
- Auto Feature Generation
- Neural Architecture Search
- Hyperparameters Optimization
- Meta Learning

## Hyperparameter Optimization (HPO)

HPOå…³æ³¨å¦‚ä½•åœ¨ç»™å®šæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„è¶…å‚æ•°ï¼Œè¿™ä¸ªè¿‡ç¨‹éå¸¸ä½¿ç”¨ä½¿ç”¨AutoMLã€‚

- `Hyperparameter configuration`æ‰¾åˆ°å›ºå®šçš„è¶…å‚æ•°è®¾ç½®ä»¥æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ã€‚
  - Random search, Grid Search
  - Successive-halving, Hyperband
  - Bayesian optimization
- `Hyperparameter schedule`åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¯»æ±‚åŠ¨æ€è¶…å‚æ•°è°ƒåº¦ã€‚
  - Population-based training
  - Hypergradient
  
  
`Hyperparameter schedule`å…³æ³¨å¦‚ä½•åœ¨å…¨å±€æœç´¢å’Œå±€éƒ¨æœç´¢ä¹‹é—´å–å¾—è‰¯å¥½çš„æƒè¡¡ã€‚


## Neural Architecture Search (NAS)

NASå…³æ³¨æ‰¾åˆ°ç¥ç»ç½‘ç»œçš„æœ€ä½³æ‹“æ‰‘å’Œç½‘ç»œé…ç½®ï¼Œåº”ç”¨çš„ä¹Ÿéå¸¸å¤§ã€‚ç°åœ¨å¾ˆå¤šCNNæ¨¡å‹éƒ½æ˜¯é€šè¿‡NASæœç´¢å¾—åˆ°ã€‚

- æœç´¢ç©ºé—´
  - ä¸ç½‘ç»œç›¸å…³çš„é…ç½®ï¼Œå¦‚ filter size, activation functions, depth
- æœç´¢ç­–ç•¥
  - How to utilize experience?
  - How to propose new configuration
  - å¦‚RL, ES, and differentiable search.
- ç²¾åº¦éªŒè¯
  - How to evaluate a configura+on?

## Meta-Learning

Meta-Learningåœ¨å…ƒæ•°æ®é›†åŒ…å«å¤šä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®é›†éƒ½æ˜¯ä¸åŒçš„ä»»åŠ¡ã€‚

## Auto Feature Generation

è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å…³æ³¨å¦‚ä½•äº§ç”Ÿæœ‰æ•ˆçš„ç‰¹å¾ï¼Œä¸”å¸Œæœ›äº§ç”Ÿçš„ç‰¹å¾èƒ½å¸¦æ¥æ›´å¥½çš„æ¨¡å‹ç²¾åº¦ã€‚

- DNN-based methodsï¼šè®¾ç½®å¯å­¦ä¹ ã€å¯äº¤å‰çš„ç½‘ç»œç»“æ„
- Search-based methodsï¼šç‰¹å¾äº¤å‰çš„ç©ºé—´æœç´¢å’Œå‰ªæã€‚

## ML-Guided Database

ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥ä¼˜åŒ–æ•°æ®åº“çš„ç´¢å¼•ã€æŸ¥è¯¢å’Œæ•°æ®åº“é…ç½®ã€‚
